{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "# ðŸŽ¬ Movie Discovery Assistant - Fine-tuning with Unsloth\n",
                "\n",
                "This notebook fine-tunes **Llama 3 (8B)** using **Unsloth**, which makes training 2x faster and uses 70% less memory. Perfect for Google Colab's free Tesla T4 GPU.\n",
                "\n",
                "**Steps:**\n",
                "1. Install Unsloth\n",
                "2. Upload your `train.jsonl`\n",
                "3. Train the model\n",
                "4. Save adapters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_unsloth"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "import torch\n",
                "major_version, minor_version = torch.cuda.get_device_capability()\n",
                "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "if major_version >= 8:\n",
                "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
                "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
                "else:\n",
                "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
                "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
                "pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_model"
            },
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
                "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
                "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
                "\n",
                "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "add_lora"
            },
            "outputs": [],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
                "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
                "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
                "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,  # We support rank stabilized LoRA\n",
                "    loftq_config = None, # And LoftQ\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "upload_data"
            },
            "source": [
                "### Upload Dataset\n",
                "Please upload your `train.jsonl` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "upload_files"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "uploaded = files.upload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "prep_dataset"
            },
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load the dataset\n",
                "dataset = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n",
                "\n",
                "# Define the formatting function\n",
                "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "{}\n",
                "\n",
                "### Input:\n",
                "{}\n",
                "\n",
                "### Response:\n",
                "{}\"\"\"\n",
                "\n",
                "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
                "def formatting_prompts_func(examples):\n",
                "    instructions = examples[\"instruction\"]\n",
                "    inputs       = examples[\"input\"]\n",
                "    outputs      = examples[\"output\"]\n",
                "    texts = []\n",
                "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
                "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
                "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched = True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train"
            },
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False, # Can make training 5x faster for short sequences.\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = 60, # Set num_train_epochs = 1 for full training\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "        output_dir = \"outputs\",\n",
                "    ),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run_training"
            },
            "outputs": [],
            "source": [
                "trainer_stats = trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "inference"
            },
            "source": [
                "### ðŸŽ® Interactive Chat (Improved)\n",
                "Run this cell to chat with your fine-tuned model! (Repetition fixes applied)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run_interactive"
            },
            "outputs": [],
            "source": [
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "print(\"ðŸŽ¬ Movie Assistant is READY! (Type 'quit' to exit)\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "while True:\n",
                "    user_query = input(\"You: \")\n",
                "    if user_query.lower() in ['quit', 'exit']:\n",
                "        break\n",
                "    \n",
                "    inputs = tokenizer(\n",
                "    [\n",
                "        alpaca_prompt.format(\n",
                "            user_query,\n",
                "            \"\", # input\n",
                "            \"\", # output\n",
                "        )\n",
                "    ], return_tensors = \"pt\").to(\"cuda\")\n",
                "\n",
                "    outputs = model.generate(\n",
                "        **inputs, \n",
                "        max_new_tokens = 256, \n",
                "        use_cache = True,\n",
                "        repetition_penalty = 1.3,  # Fixes repetition\n",
                "        no_repeat_ngram_size = 2,  # Prevents 2-word phrase loops\n",
                "        temperature = 0.9,         # More creativity\n",
                "        top_k = 50,                # Diversity\n",
                "        top_p = 0.95\n",
                "    )\n",
                "    response = tokenizer.batch_decode(outputs)[0]\n",
                "    \n",
                "    # Clean up response to show only the assistant's part\n",
                "    response_clean = response.split(\"### Response:\")[-1].replace(tokenizer.eos_token, \"\").strip()\n",
                "    \n",
                "    print(f\"Assistant: {response_clean}\")\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "save_model"
            },
            "source": [
                "### Save Model (Manual Download)\n",
                "If the API fails, run this cell to download the model as a ZIP file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "manual_download"
            },
            "outputs": [],
            "source": [
                "# Option 2: Manual Download (If API fails)\n",
                "import shutil\n",
                "from google.colab import files\n",
                "\n",
                "# Save locally first\n",
                "model.save_pretrained(\"lora_model\")\n",
                "tokenizer.save_pretrained(\"lora_model\")\n",
                "\n",
                "# Zip the folder\n",
                "shutil.make_archive('my_movie_model', 'zip', 'lora_model')\n",
                "\n",
                "# Download it\n",
                "files.download('my_movie_model.zip')\n",
                "print(\"âœ… Download started! Unzip this and upload files to https://huggingface.co/new\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}