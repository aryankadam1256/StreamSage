{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéâ Smart Recommender ‚Äì Sentence‚ÄëTransformer (SBERT)\n",
                "\n",
                "In the previous notebook we built a TF‚ÄëIDF baseline.  \n",
                "Now we‚Äôll replace the **bag‚Äëof‚Äëwords** representation with **dense semantic embeddings** from a pretrained sentence‚Äëtransformer.  \n",
                "The workflow is exactly the same as before, only the vectorisation step changes:\n",
                "1Ô∏è‚É£ Install `sentence‚Äëtransformers`.  \n",
                "2Ô∏è‚É£ Load a lightweight model (`all‚ÄëMiniLM‚ÄëL6‚Äëv2`).  \n",
                "3Ô∏è‚É£ Encode every movie‚Äôs combined text into a 384‚Äëdim vector.  \n",
                "4Ô∏è‚É£ At query time we encode the user‚Äôs sentence and compute cosine similarity against all movie vectors.  \n",
                "5Ô∏è‚É£ Return the top‚ÄëN most similar movies.\n",
                "\n",
                "Because the model already knows **semantic relationships** (e.g., *dream* ‚âà *sleep*, *heist* ‚âà *robbery*), the recommendations become **much more relevant** than the TF‚ÄëIDF baseline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1Ô∏è‚É£ Install the library (run once)\n",
                "!pip install -q sentence-transformers\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "print(\"‚úÖ sentence‚Äëtransformers installed and imports ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÇ Load the **cleaned** TMDb data\n",
                "\n",
                "We reuse the `movies_clean` DataFrame you created in the TF‚ÄëIDF notebook.  \n",
                "If you are running this notebook **after** the previous one in the same session, the variable already exists.  \n",
                "Otherwise, just re‚Äërun the loading cells from `01_tmdb_exploration.ipynb` (they are tiny)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the CSV we saved at the end of the TF‚ÄëIDF notebook\n",
                "movies_clean = pd.read_csv('tmdb_clean.csv')\n",
                "\n",
                "# Re‚Äëcreate the combined text column (just in case we started a fresh kernel)\n",
                "def create_combined_text(row):\n",
                "    parts = [str(row['overview'])]\n",
                "    if row['genres_list']:\n",
                "        parts.append(' '.join(row['genres_list']))\n",
                "    if row['keywords_list']:\n",
                "        parts.append(' '.join(row['keywords_list']))\n",
                "    return ' '.join(parts)\n",
                "\n",
                "movies_clean['combined_text'] = movies_clean.apply(create_combined_text, axis=1)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(movies_clean)} movies with combined text ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ü§ñ 2Ô∏è‚É£ Load a pretrained sentence‚Äëtransformer\n",
                "\n",
                "We pick **`all‚ÄëMiniLM‚ÄëL6‚Äëv2`** ‚Äì 384‚Äëdim, ~30‚ÄØMB, works well on CPUs and GPUs.  \n",
                "If you have a GPU you‚Äôll see a speed boost, but the model runs fine on a regular Colab CPU runtime."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the model (download ~30‚ÄØMB on first run)\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "print(\"‚úÖ Model loaded! Vocabulary size:\", len(model.get_vocab()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ 3Ô∏è‚É£ Encode **all** movies once (this is the heavy step)\n",
                "\n",
                "We turn every `combined_text` into a 384‚Äëdim dense vector and store it in a NumPy array.  \n",
                "The resulting matrix has shape `(num_movies, 384)` ‚Äì tiny compared to the original TF‚ÄëIDF matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode all movies ‚Äì this may take ~30‚Äë45‚ÄØseconds for ~5k rows\n",
                "movie_embeddings = model.encode(\n",
                "    movies_clean['combined_text'].tolist(),\n",
                "    batch_size=64,\n",
                "    show_progress_bar=True,\n",
                "    convert_to_numpy=True\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Encoded {movie_embeddings.shape[0]} movies ‚Üí {movie_embeddings.shape[1]}‚Äëdim vectors\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîé 4Ô∏è‚É£ Recommendation function (query ‚Üí top‚ÄëN movies)\n",
                "\n",
                "The function does three things:\n",
                "1Ô∏è‚É£ Encode the user query with the same model.\n",
                "2Ô∏è‚É£ Compute **cosine similarity** against the pre‚Äëcomputed movie embeddings.\n",
                "3Ô∏è‚É£ Return the `top_n` titles with their similarity scores (0‚Äë1).\n",
                "\n",
                "Because the embeddings are dense, cosine similarity is a **single matrix multiplication** ‚Äì extremely fast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def recommend_by_embedding(query: str, top_n: int = 5):\n",
                "    \"\"\"Return the most similar movies for a free‚Äëtext query.\n",
                "    \n",
                "    Parameters\n",
                "    ----------\n",
                "    query : str\n",
                "        User‚Äôs natural‚Äëlanguage request (e.g., \"mind‚Äëbending sci‚Äëfi with dreams\").\n",
                "    top_n : int, optional\n",
                "        Number of movies to return (default 5).\n",
                "    \"\"\"\n",
                "    # 1Ô∏è‚É£ Encode the query\n",
                "    q_vec = model.encode([query], convert_to_numpy=True)\n",
                "\n",
                "    # 2Ô∏è‚É£ Cosine similarity against all movie vectors\n",
                "    sims = cosine_similarity(q_vec, movie_embeddings).flatten()\n",
                "\n",
                "    # 3Ô∏è‚É£ Get top‚ÄëN indices (largest similarity first)\n",
                "    top_idx = sims.argsort()[::-1][:top_n]\n",
                "\n",
                "    # 4Ô∏è‚É£ Build a friendly output list\n",
                "    results = []\n",
                "    for i in top_idx:\n",
                "        title = movies_clean.iloc[i]['title']\n",
                "        score = round(sims[i], 4)\n",
                "        results.append((title, score))\n",
                "    return results\n",
                "\n",
                "# Quick sanity check\n",
                "print(\"üîé Test query ‚Üí results:\")\n",
                "for t, s in recommend_by_embedding('mind‚Äëbending sci‚Äëfi with dreams'):\n",
                "    print(f\"  ‚Ä¢ {t} (score={s})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 5Ô∏è‚É£ Quick evaluation ‚Äì try a few different queries\n",
                "\n",
                "Run the cell below and replace the strings with any natural‚Äëlanguage request you like.  \n",
                "You should see **semantically relevant** movies (e.g., *Inception*, *Paprika*, *The Matrix*, *Interstellar*, etc.)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "queries = [\n",
                "    \"mind‚Äëbending sci‚Äëfi with dreams\",\n",
                "    \"family friendly animated adventure\",\n",
                "    \"dark thriller about a detective\",\n",
                "    \"movie about time travel and paradoxes\",\n",
                "    \"light romantic comedy set in Paris\"\n",
                "]\n",
                "\n",
                "for q in queries:\n",
                "    print(f\"\\nüîé Query: '{q}'\")\n",
                "    for title, score in recommend_by_embedding(q, top_n=5):\n",
                "        print(f\"  ‚Ä¢ {title} (score={score})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ 6Ô∏è‚É£ Persist the embeddings (optional but handy for production)\n",
                "\n",
                "If you plan to serve the recommender from a web service, you don‚Äôt want to re‚Äëencode all movies on every start‚Äëup.  \n",
                "Save the dense matrix and the title list to disk ‚Äì they can be loaded instantly later.\n",
                "\n",
                "```python\n",
                "# Save\n",
                "np.save('movie_embeddings.npy', movie_embeddings)\n",
                "movies_clean[['title']].to_csv('movie_titles.csv', index=False)\n",
                "\n",
                "# Load (in a new session)\n",
                "movie_embeddings = np.load('movie_embeddings.npy')\n",
                "titles = pd.read_csv('movie_titles.csv')['title'].tolist()\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ 7Ô∏è‚É£ Next step ‚Äì expose the recommender as a tiny API (FastAPI)\n",
                "\n",
                "Below is a **minimal FastAPI** server you can copy into `services/recommender-service/main.py`.  \n",
                "It loads the saved embeddings and answers `POST /recommend` with JSON.\n",
                "\n",
                "```python\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "app = FastAPI()\n",
                "\n",
                "class RecRequest(BaseModel):\n",
                "    query: str\n",
                "    top_n: int = 5\n",
                "\n",
                "# Load model & data once at startup\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "movie_embeddings = np.load('movie_embeddings.npy')\n",
                "titles = pd.read_csv('movie_titles.csv')['title'].tolist()\n",
                "\n",
                "@app.post('/recommend')\n",
                "def recommend(req: RecRequest):\n",
                "    q_vec = model.encode([req.query], convert_to_numpy=True)\n",
                "    sims = cosine_similarity(q_vec, movie_embeddings).flatten()\n",
                "    top_idx = sims.argsort()[::-1][:req.top_n]\n",
                "    results = [{\"title\": titles[i], \"score\": round(float(sims[i]), 4)} for i in top_idx]\n",
                "    return {\"query\": req.query, \"results\": results}\n",
                "```\n",
                "\n",
                "Run the service with Docker (or `uvicorn main:app --host 0.0.0.0 --port 8000`).  \n",
                "Your frontend can now call `POST /recommend` and get instant, semantically‚Äëaware suggestions.\n",
                "\n",
                "---\n",
                "### üéâ You‚Äôre done!\n",
                "\n",
                "You now have:\n",
                "1. **A TF‚ÄëIDF baseline** (already built).\n",
                "2. **A smart SBERT‚Äëbased recommender** that understands meaning.\n",
                "3. **Persistence** of embeddings for fast start‚Äëup.\n",
                "4. **A minimal FastAPI wrapper** ready to be containerised.\n",
                "\n",
                "Feel free to experiment:\n",
                "- Try a larger model (`all‚Äëdistilroberta‚Äëv1`) for even richer semantics.\n",
                "- Fine‚Äëtune the sentence‚Äëtransformer on your own movie‚Äëspecific corpus (optional).\n",
                "- Add genre/keyword weighting in the final similarity (e.g., `0.6*embed + 0.4*tfidf`).\n",
                "\n",
                "Enjoy building the **Movie Discovery Assistant**! üöÄ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}