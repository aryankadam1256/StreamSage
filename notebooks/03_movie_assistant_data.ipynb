{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¨ Movie Discovery Assistant - Data Preparation\n",
                "\n",
                "**Goal**: Prepare TMDb data for RAG by creating a vector database.\n",
                "\n",
                "## What We'll Do\n",
                "1. Load TMDb 5000 dataset from Kaggle\n",
                "2. Parse JSON fields (genres, keywords, cast)\n",
                "3. Create enriched \"documents\" for each movie\n",
                "4. Generate embeddings using sentence-transformers\n",
                "5. Store in ChromaDB for fast semantic search\n",
                "6. Download the ChromaDB folder for local use\n",
                "\n",
                "---\n",
                "\n",
                "## üéì Key Concepts\n",
                "\n",
                "### What is a \"Document\" in RAG?\n",
                "For each movie, we create a text document that combines:\n",
                "- **Title**: \"Inception\"\n",
                "- **Plot**: \"A thief who steals corporate secrets through dream-sharing...\"\n",
                "- **Genres**: \"Action, Sci-Fi, Thriller\"\n",
                "- **Keywords**: \"dream, subconscious, heist\"\n",
                "\n",
                "This becomes: `\"Inception. A thief who steals corporate secrets... Action Sci-Fi Thriller dream subconscious heist\"`\n",
                "\n",
                "### Why Combine Fields?\n",
                "The embedding model needs **context**. Just \"Inception\" doesn't tell us much. But the full document captures:\n",
                "- Semantic meaning (what the movie is about)\n",
                "- Genre signals (for filtering)\n",
                "- Thematic keywords (for similarity)\n",
                "\n",
                "### What are Embeddings?\n",
                "Embeddings convert text to vectors (lists of numbers). Similar movies get similar vectors.\n",
                "\n",
                "Example:\n",
                "- \"Inception\" ‚Üí [0.23, -0.45, 0.78, ...] (384 numbers)\n",
                "- \"Interstellar\" ‚Üí [0.25, -0.43, 0.81, ...] (very close!)\n",
                "- \"Toy Story\" ‚Üí [-0.12, 0.67, -0.34, ...] (far away)\n",
                "\n",
                "---\n",
                "\n",
                "Let's start! üöÄ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 1: Install Required Libraries\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Dependencies\n",
                "#\n",
                "# - chromadb: Vector database (stores embeddings)\n",
                "# - sentence-transformers: Creates embeddings locally\n",
                "# - pandas: Data manipulation\n",
                "#\n",
                "# Why ChromaDB?\n",
                "# - Works in-memory (no server setup)\n",
                "# - Can persist to disk (save for later)\n",
                "# - Built for embeddings (not like PostgreSQL)\n",
                "# ============================================================================\n",
                "\n",
                "!pip install -q chromadb sentence-transformers pandas numpy\n",
                "\n",
                "print(\"‚úÖ Libraries installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 2: Download TMDb Dataset from Kaggle API\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Kaggle API\n",
                "#\n",
                "# Kaggle provides an API to download datasets programmatically.\n",
                "# You need to upload your kaggle.json credentials file first.\n",
                "#\n",
                "# How to get kaggle.json:\n",
                "# 1. Go to kaggle.com ‚Üí Account ‚Üí Create New API Token\n",
                "# 2. Download kaggle.json\n",
                "# 3. Upload it to Colab when prompted below\n",
                "# ============================================================================\n",
                "\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Upload kaggle.json\n",
                "print(\"üìÅ Please upload your kaggle.json file:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Set up Kaggle credentials\n",
                "!mkdir -p ~/.kaggle\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "\n",
                "# Download TMDb dataset\n",
                "print(\"‚¨áÔ∏è Downloading TMDb 5000 dataset...\")\n",
                "!kaggle datasets download -d tmdb/tmdb-movie-metadata\n",
                "!unzip -q tmdb-movie-metadata.zip\n",
                "\n",
                "print(\"‚úÖ Dataset downloaded!\")\n",
                "!ls -lh *.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 3: Load and Explore the Data\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Data Loading\n",
                "#\n",
                "# TMDb provides two CSVs:\n",
                "# - tmdb_5000_movies.csv: Movie metadata (title, plot, budget, revenue)\n",
                "# - tmdb_5000_credits.csv: Cast and crew (actors, directors)\n",
                "#\n",
                "# We'll merge these on 'title' to get complete information.\n",
                "# ============================================================================\n",
                "\n",
                "import pandas as pd\n",
                "import json\n",
                "from ast import literal_eval\n",
                "\n",
                "# Load datasets\n",
                "movies = pd.read_csv('tmdb_5000_movies.csv')\n",
                "credits = pd.read_csv('tmdb_5000_credits.csv')\n",
                "\n",
                "print(f\"üìä Loaded {len(movies)} movies\")\n",
                "print(f\"üìä Loaded {len(credits)} credit records\\n\")\n",
                "\n",
                "# Preview the data\n",
                "print(\"üîç Movie columns:\")\n",
                "print(movies.columns.tolist())\n",
                "print(\"\\nüîç First movie:\")\n",
                "movies.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 4: Parse JSON Fields\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: JSON Parsing in Pandas\n",
                "#\n",
                "# TMDb stores complex fields as JSON strings:\n",
                "# genres: '[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}]'\n",
                "#\n",
                "# We need to:\n",
                "# 1. Parse the JSON string ‚Üí Python list\n",
                "# 2. Extract just the 'name' field\n",
                "# 3. Join into a single string: \"Action Adventure\"\n",
                "#\n",
                "# Why?\n",
                "# The embedding model works better with text than structured data.\n",
                "# ============================================================================\n",
                "\n",
                "def parse_json_field(field, key='name', limit=5):\n",
                "    \"\"\"\n",
                "    Parse a JSON field and extract specific key.\n",
                "    \n",
                "    Args:\n",
                "        field: JSON string like '[{\"name\": \"Action\"}]'\n",
                "        key: Which key to extract (default: 'name')\n",
                "        limit: Maximum items to extract\n",
                "    \n",
                "    Returns:\n",
                "        List of strings: ['Action', 'Adventure']\n",
                "    \"\"\"\n",
                "    if pd.isna(field):\n",
                "        return []\n",
                "    try:\n",
                "        parsed = literal_eval(field)  # Safe eval for Python literals\n",
                "        return [item[key] for item in parsed[:limit]]\n",
                "    except:\n",
                "        return []\n",
                "\n",
                "# Parse genres\n",
                "movies['genres_list'] = movies['genres'].apply(parse_json_field)\n",
                "\n",
                "# Parse keywords\n",
                "movies['keywords_list'] = movies['keywords'].apply(parse_json_field, limit=10)\n",
                "\n",
                "# Parse production companies\n",
                "movies['companies_list'] = movies['production_companies'].apply(parse_json_field, limit=3)\n",
                "\n",
                "print(\"‚úÖ Parsed JSON fields!\")\n",
                "print(\"\\nüîç Example:\")\n",
                "print(f\"Title: {movies.iloc[0]['title']}\")\n",
                "print(f\"Genres: {movies.iloc[0]['genres_list']}\")\n",
                "print(f\"Keywords: {movies.iloc[0]['keywords_list']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 5: Merge with Credits Data\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Data Merging\n",
                "#\n",
                "# We merge movies + credits to get:\n",
                "# - Cast (top 5 actors)\n",
                "# - Director\n",
                "#\n",
                "# This allows queries like:\n",
                "# \"Movies with Tom Hanks\"\n",
                "# \"Christopher Nolan films\"\n",
                "# ============================================================================\n",
                "\n",
                "# Parse cast (top 5 actors)\n",
                "credits['cast_list'] = credits['cast'].apply(parse_json_field, limit=5)\n",
                "\n",
                "# Parse crew to get director\n",
                "def get_director(crew_str):\n",
                "    \"\"\"Extract director from crew JSON.\"\"\"\n",
                "    if pd.isna(crew_str):\n",
                "        return None\n",
                "    try:\n",
                "        crew = literal_eval(crew_str)\n",
                "        for person in crew:\n",
                "            if person.get('job') == 'Director':\n",
                "                return person.get('name')\n",
                "    except:\n",
                "        pass\n",
                "    return None\n",
                "\n",
                "credits['director'] = credits['crew'].apply(get_director)\n",
                "\n",
                "# Merge on title\n",
                "movies_full = movies.merge(\n",
                "    credits[['title', 'cast_list', 'director']],\n",
                "    on='title',\n",
                "    how='left'\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Merged data: {len(movies_full)} movies with cast & crew\")\n",
                "print(\"\\nüîç Example:\")\n",
                "sample = movies_full.iloc[0]\n",
                "print(f\"Title: {sample['title']}\")\n",
                "print(f\"Director: {sample['director']}\")\n",
                "print(f\"Cast: {sample['cast_list']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 6: Create Enriched Documents\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Document Construction for RAG\n",
                "#\n",
                "# Each movie becomes a single text document with ALL relevant info.\n",
                "#\n",
                "# Structure:\n",
                "# Title: [Movie Name]\n",
                "# Plot: [Overview]\n",
                "# Genres: [Genre1, Genre2]\n",
                "# Keywords: [Keyword1, Keyword2]\n",
                "# Director: [Name]\n",
                "# Cast: [Actor1, Actor2]\n",
                "#\n",
                "# Why this format?\n",
                "# - Clear section headers help the LLM understand context\n",
                "# - Natural language (not just concatenated words)\n",
                "# - Embeddings capture both semantic and structural info\n",
                "# ============================================================================\n",
                "\n",
                "def create_movie_document(row):\n",
                "    \"\"\"\n",
                "    Create a rich text document for a movie.\n",
                "    \n",
                "    This becomes the \"content\" that gets embedded.\n",
                "    \"\"\"\n",
                "    parts = []\n",
                "    \n",
                "    # Title (always include)\n",
                "    parts.append(f\"Title: {row['title']}\")\n",
                "    \n",
                "    # Plot/Overview\n",
                "    if pd.notna(row['overview']) and row['overview'].strip():\n",
                "        parts.append(f\"Plot: {row['overview']}\")\n",
                "    \n",
                "    # Genres\n",
                "    if row['genres_list']:\n",
                "        parts.append(f\"Genres: {', '.join(row['genres_list'])}\")\n",
                "    \n",
                "    # Keywords\n",
                "    if row['keywords_list']:\n",
                "        parts.append(f\"Keywords: {', '.join(row['keywords_list'])}\")\n",
                "    \n",
                "    # Director\n",
                "    if pd.notna(row['director']):\n",
                "        parts.append(f\"Director: {row['director']}\")\n",
                "    \n",
                "    # Cast\n",
                "    if row['cast_list']:\n",
                "        parts.append(f\"Cast: {', '.join(row['cast_list'])}\")\n",
                "    \n",
                "    # Release year (helpful for filtering)\n",
                "    if pd.notna(row['release_date']):\n",
                "        year = row['release_date'][:4]\n",
                "        parts.append(f\"Year: {year}\")\n",
                "    \n",
                "    return \"\\n\".join(parts)\n",
                "\n",
                "# Create documents\n",
                "movies_full['document'] = movies_full.apply(create_movie_document, axis=1)\n",
                "\n",
                "# Filter out movies without overview (empty documents)\n",
                "movies_clean = movies_full[movies_full['overview'].notna()].copy()\n",
                "movies_clean = movies_clean.reset_index(drop=True)\n",
                "\n",
                "print(f\"‚úÖ Created {len(movies_clean)} documents\\n\")\n",
                "print(\"üîç Example document:\")\n",
                "print(\"=\" * 60)\n",
                "print(movies_clean.iloc[0]['document'])\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 7: Prepare Metadata for ChromaDB\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Metadata in Vector Databases\n",
                "#\n",
                "# ChromaDB stores:\n",
                "# 1. Document text (the enriched string)\n",
                "# 2. Embedding vector (384 numbers)\n",
                "# 3. Metadata (structured info for filtering)\n",
                "#\n",
                "# Metadata allows queries like:\n",
                "# \"Recommend a horror movie from 2015-2020\"\n",
                "#\n",
                "# ChromaDB can filter by metadata BEFORE doing semantic search.\n",
                "# This is MUCH faster than searching all 5000 movies!\n",
                "# ============================================================================\n",
                "\n",
                "def create_metadata(row):\n",
                "    \"\"\"\n",
                "    Extract metadata for filtering.\n",
                "    \n",
                "    Note: ChromaDB only supports:\n",
                "    - Strings\n",
                "    - Numbers (int, float)\n",
                "    - Booleans\n",
                "    \n",
                "    No lists/dicts directly. We'll convert lists to comma-separated strings.\n",
                "    \"\"\"\n",
                "    meta = {\n",
                "        'title': row['title'],\n",
                "        'movie_id': str(row['id']),\n",
                "    }\n",
                "    \n",
                "    # Genres (as string for filtering)\n",
                "    if row['genres_list']:\n",
                "        meta['genres'] = ','.join(row['genres_list'])\n",
                "    \n",
                "    # Year (as integer for range filtering)\n",
                "    if pd.notna(row['release_date']):\n",
                "        try:\n",
                "            meta['year'] = int(row['release_date'][:4])\n",
                "        except:\n",
                "            pass\n",
                "    \n",
                "    # Rating\n",
                "    if pd.notna(row['vote_average']):\n",
                "        meta['rating'] = float(row['vote_average'])\n",
                "    \n",
                "    # Director\n",
                "    if pd.notna(row['director']):\n",
                "        meta['director'] = row['director']\n",
                "    \n",
                "    return meta\n",
                "\n",
                "# Create metadata for all movies\n",
                "metadatas = movies_clean.apply(create_metadata, axis=1).tolist()\n",
                "\n",
                "print(\"‚úÖ Created metadata for all movies\\n\")\n",
                "print(\"üîç Example metadata:\")\n",
                "print(metadatas[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 8: Initialize ChromaDB and Embedding Model\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Embedding Models\n",
                "#\n",
                "# all-MiniLM-L6-v2 is a sentence-transformer model:\n",
                "# - Input: Text of any length\n",
                "# - Output: 384-dimensional vector\n",
                "#\n",
                "# Why this model?\n",
                "# - Fast: 14,000 sentences/second on CPU\n",
                "# - Small: 80MB download\n",
                "# - Quality: Trained on 1 billion sentence pairs\n",
                "#\n",
                "# Alternatives:\n",
                "# - all-mpnet-base-v2: Better quality, slower (768-dim)\n",
                "# - all-distilroberta-v1: Good balance (768-dim)\n",
                "# ============================================================================\n",
                "\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Initialize embedding model\n",
                "print(\"üìä Loading embedding model: all-MiniLM-L6-v2\")\n",
                "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "print(\"‚úÖ Model loaded!\\n\")\n",
                "\n",
                "# Initialize ChromaDB (in-memory for now)\n",
                "print(\"üóÑÔ∏è Initializing ChromaDB...\")\n",
                "chroma_client = chromadb.Client(Settings(\n",
                "    chroma_db_impl=\"duckdb+parquet\",\n",
                "    persist_directory=\"./chroma_db\"  # Save to disk\n",
                "))\n",
                "\n",
                "# Create or get collection\n",
                "# If collection exists, delete it (fresh start)\n",
                "try:\n",
                "    chroma_client.delete_collection(\"movies\")\n",
                "except:\n",
                "    pass\n",
                "\n",
                "collection = chroma_client.create_collection(\n",
                "    name=\"movies\",\n",
                "    metadata={\"description\": \"TMDb 5000 movies for RAG\"}\n",
                ")\n",
                "\n",
                "print(\"‚úÖ ChromaDB initialized!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 9: Generate Embeddings and Ingest into ChromaDB\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Batch Processing\n",
                "#\n",
                "# We have ~4800 movies. Encoding them one-by-one would be slow.\n",
                "#\n",
                "# batch_size=32 means:\n",
                "# - Process 32 movies at once\n",
                "# - GPU can parallelize this (if available)\n",
                "# - Much faster than one-at-a-time\n",
                "#\n",
                "# Why not batch_size=4800?\n",
                "# - Would exceed GPU memory\n",
                "# - Diminishing returns after ~64\n",
                "#\n",
                "# This step takes ~2-3 minutes on Colab's free tier.\n",
                "# ============================================================================\n",
                "\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"üöÄ Generating embeddings and ingesting into ChromaDB...\")\n",
                "print(f\"   Total movies: {len(movies_clean)}\")\n",
                "print(\"   This will take ~2-3 minutes...\\n\")\n",
                "\n",
                "# Batch size for ingestion\n",
                "BATCH_SIZE = 100\n",
                "\n",
                "documents = movies_clean['document'].tolist()\n",
                "ids = [f\"movie_{i}\" for i in range(len(documents))]\n",
                "\n",
                "# Process in batches\n",
                "for i in tqdm(range(0, len(documents), BATCH_SIZE)):\n",
                "    batch_docs = documents[i:i+BATCH_SIZE]\n",
                "    batch_ids = ids[i:i+BATCH_SIZE]\n",
                "    batch_meta = metadatas[i:i+BATCH_SIZE]\n",
                "    \n",
                "    # Generate embeddings\n",
                "    embeddings = embedding_model.encode(\n",
                "        batch_docs,\n",
                "        convert_to_numpy=True,\n",
                "        show_progress_bar=False\n",
                "    ).tolist()\n",
                "    \n",
                "    # Add to ChromaDB\n",
                "    collection.add(\n",
                "        ids=batch_ids,\n",
                "        documents=batch_docs,\n",
                "        embeddings=embeddings,\n",
                "        metadatas=batch_meta\n",
                "    )\n",
                "\n",
                "print(f\"\\n‚úÖ Ingested {collection.count()} movies into ChromaDB!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 10: Test the Vector Database\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Semantic Search\n",
                "#\n",
                "# Let's test if our embeddings work!\n",
                "#\n",
                "# We'll query: \"mind-bending sci-fi about dreams\"\n",
                "#\n",
                "# The database will:\n",
                "# 1. Embed the query ‚Üí vector\n",
                "# 2. Compare to all movie vectors (cosine similarity)\n",
                "# 3. Return top-K most similar\n",
                "#\n",
                "# Expected result: Inception, Paprika, The Matrix, Interstellar\n",
                "# ============================================================================\n",
                "\n",
                "def test_query(query_text, n_results=5):\n",
                "    \"\"\"Test semantic search.\"\"\"\n",
                "    print(f\"üîç Query: '{query_text}'\\n\")\n",
                "    \n",
                "    # Embed query\n",
                "    query_embedding = embedding_model.encode([query_text]).tolist()\n",
                "    \n",
                "    # Search ChromaDB\n",
                "    results = collection.query(\n",
                "        query_embeddings=query_embedding,\n",
                "        n_results=n_results\n",
                "    )\n",
                "    \n",
                "    # Display results\n",
                "    print(\"üìã Top Results:\")\n",
                "    for i, (doc, meta, dist) in enumerate(zip(\n",
                "        results['documents'][0],\n",
                "        results['metadatas'][0],\n",
                "        results['distances'][0]\n",
                "    )):\n",
                "        print(f\"\\n{i+1}. {meta['title']}\")\n",
                "        print(f\"   Similarity: {1 - dist:.3f}\")  # Convert distance to similarity\n",
                "        print(f\"   Year: {meta.get('year', 'N/A')} | Rating: {meta.get('rating', 'N/A')}\")\n",
                "        print(f\"   Genres: {meta.get('genres', 'N/A')}\")\n",
                "\n",
                "# Test queries\n",
                "test_query(\"mind-bending sci-fi about dreams\")\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "test_query(\"family-friendly animated adventure\")\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "test_query(\"dark thriller with a detective\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 11: Persist ChromaDB to Disk\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Persistence\n",
                "#\n",
                "# ChromaDB auto-saves to 'persist_directory' on every add().\n",
                "# But we'll explicitly persist to be safe.\n",
                "#\n",
                "# The 'chroma_db' folder contains:\n",
                "# - Parquet files (embeddings)\n",
                "# - DuckDB index (for fast search)\n",
                "# - Metadata (schemas, config)\n",
                "#\n",
                "# This folder is ~50MB (compressed embeddings).\n",
                "# ============================================================================\n",
                "\n",
                "# Ensure data is persisted\n",
                "chroma_client.persist()\n",
                "\n",
                "print(\"‚úÖ ChromaDB persisted to ./chroma_db\")\n",
                "print(\"\\nüìÅ Folder contents:\")\n",
                "!ls -lh chroma_db/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Step 12: Download ChromaDB for Local Use\n",
                "# ============================================================================\n",
                "# üéì CONCEPT: Transferring Data from Colab\n",
                "#\n",
                "# We need to download the 'chroma_db' folder to use locally.\n",
                "#\n",
                "# Steps:\n",
                "# 1. Zip the folder\n",
                "# 2. Download the zip file\n",
                "# 3. Extract it in your local project\n",
                "# ============================================================================\n",
                "\n",
                "# Zip the ChromaDB folder\n",
                "!zip -r chroma_db.zip chroma_db/\n",
                "\n",
                "print(\"üì¶ ChromaDB zipped!\")\n",
                "!ls -lh chroma_db.zip\n",
                "\n",
                "# Download the file\n",
                "print(\"\\n‚¨áÔ∏è Downloading chroma_db.zip...\")\n",
                "files.download('chroma_db.zip')\n",
                "\n",
                "print(\"\\n‚úÖ Download complete!\")\n",
                "print(\"\\nüìã Next Steps:\")\n",
                "print(\"1. Extract chroma_db.zip in your local project\")\n",
                "print(\"2. Place it at: d:/PROJECTS/StreamSage/data/chroma_db\")\n",
                "print(\"3. We'll use this in the RAG service!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéâ Congratulations!\n",
                "\n",
                "You've successfully:\n",
                "1. ‚úÖ Loaded TMDb data\n",
                "2. ‚úÖ Parsed complex JSON fields\n",
                "3. ‚úÖ Created enriched documents\n",
                "4. ‚úÖ Generated embeddings for 4800+ movies\n",
                "5. ‚úÖ Ingested into ChromaDB\n",
                "6. ‚úÖ Tested semantic search\n",
                "7. ‚úÖ Downloaded for local use\n",
                "\n",
                "---\n",
                "\n",
                "## üéì What You Learned\n",
                "\n",
                "### 1. Document Construction\n",
                "- Why we combine multiple fields into one text\n",
                "- How structured metadata enables filtering\n",
                "- Balance between context and noise\n",
                "\n",
                "### 2. Embeddings\n",
                "- What embeddings are (text ‚Üí vectors)\n",
                "- Why similar content gets similar vectors\n",
                "- Trade-offs between model size and quality\n",
                "\n",
                "### 3. Vector Databases\n",
                "- How ChromaDB stores and searches embeddings\n",
                "- Metadata filtering vs semantic search\n",
                "- Batch processing for efficiency\n",
                "\n",
                "---\n",
                "\n",
                "## üöÄ Next Steps\n",
                "\n",
                "Now that we have the vector database ready, we'll build the **RAG Service** locally:\n",
                "\n",
                "1. Load this ChromaDB\n",
                "2. Connect to Mistral LLM (via Ollama)\n",
                "3. Create a FastAPI service\n",
                "4. Test conversational recommendations\n",
                "\n",
                "Let's do it! üé¨"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}